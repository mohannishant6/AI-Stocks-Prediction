# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MSmKdi8P2i_eXuNfqBgl0fWy7rHgmJqM
"""
src = "stocks/"

import itertools
import numpy as np
import matplotlib.pyplot as plt
# warnings.filterwarnings("ignore")
plt.style.use('fivethirtyeight')
import pandas as pd
import statsmodels.api as sm
import matplotlib
from pylab import rcParams
from pmdarima.arima import auto_arima
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM

matplotlib.rcParams['axes.labelsize'] = 14
matplotlib.rcParams['xtick.labelsize'] = 12
matplotlib.rcParams['ytick.labelsize'] = 12
matplotlib.rcParams['text.color'] = 'k'

def read_stock_data(path):
    df = pd.read_csv(path)
    df = df.sort_values('Date')
    return df.dropna()

def create_decision_data(df, date, ls):
    
#     df = df[df['Date'] <= date]
    
    dl = [['Stock Symbol','last_day_value','next_day_forecast','AIC','next_day_value']]

    for i in ls[0:1]: #### Change later
        # try:
        df2 = df.loc[df['Stock Symbol'] == i].drop(columns = 'Stock Symbol')
        # df2.isnull().sum()
        df2['Date'] = pd.to_datetime(df2['Date'], format='%Y-%m-%d', errors='coerce')
        # df['Date'] = pd.to_datetime(df.Date,format='%Y-%m-%d')
        df2.index = df2['Date']
        train_length=len(df2['Date'] <= date)
        # val_length=len([df2['Date'] > date])
        # print (len(df2['Date'] <= date))

        #creating dataframe
        data = df2.sort_index(ascending=True, axis=0)
        new_data = pd.DataFrame(index=range(0,len(df)),columns=['Date', 'Close'])
        for i in range(0,len(data)):
            new_data['Date'][i] = data['Date'][i]
            new_data['Close'][i] = data['Close'][i]

        #setting index
        new_data.index = new_data.Date
        new_data.drop('Date', axis=1, inplace=True)

        #creating train and test sets
        dataset = new_data.values

        train = dataset[0:train_length,:]
        valid = dataset[train_length:,:]
        # print(train.shape, valid.shape)

        #converting dataset into x_train and y_train
        scaler = MinMaxScaler(feature_range=(0, 1))
        scaled_data = scaler.fit_transform(dataset)

        x_train, y_train = [], []
        for i in range(60,len(train)):
            x_train.append(scaled_data[i-60:i,0])
            y_train.append(scaled_data[i,0])
        x_train, y_train = np.array(x_train), np.array(y_train)


        # print(x_train.shape,y_train.shape)
        x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))

        # create and fit the LSTM network
        model = Sequential()
        model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],1)))
        model.add(LSTM(units=50))
        model.add(Dense(1))

        model.compile(loss='mean_squared_error', optimizer='adam')
        model.fit(x_train, y_train, epochs=1, batch_size=1, verbose=2)

        #predicting 246 values, using past 60 from the train data
        inputs = new_data[len(new_data) - len(valid) - 60:].values
        inputs = inputs.reshape(-1,1)
        inputs  = scaler.transform(inputs)

        X_test = []
        for i in range(60,inputs.shape[0]):
            X_test.append(inputs[i-60:i,0])
        X_test = np.array(X_test)

        X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))
        closing_price = model.predict(X_test)
        forecast = scaler.inverse_transform(closing_price)
        confidence=model.predict_proba(X_test)



        # close_prices = model.make_future_dataframe(periods=len(df2v))
        # forecast = model.predict(close_prices)

        next_day_forecast = forecast[['yhat','ds']][:1]


        next_day_forecast = forecast[0,0]
        confidence_next_day = confidence[0,0]
        next_day_value = valid[:1,0]
        last_day_value = train[-1:,0]
        dl.append([i, last_day_value, next_day_forecast, confidence_next_day, next_day_value])
        # except:
        #     print('Error with ',i)

    df3 = pd.DataFrame(dl)
    df3.columns = df3.iloc[0]
    df3 = df3.drop(df3.index[0])
    df3['diff'] = df3['next_day_forecast'] - df3['last_day_value']
    df3 = df3.sort_values('diff', ascending = False)
    return df3

def update_ledger(df3, path):
    try:
        dfl = pd.read_csv(path)
        dfl2 = dfl.merge(df3, on = 'Stock Symbol')
        dfl2['Investment'] = dfl2['Investment']*(dfl2['last_day_value']/dfl2['Bought at'])
        dfl2['Bought at'] = dfl2['last_day_value']
        dfl2 = dfl2[['Stock Symbol','Bought at','Investment']]
    except:
        dfl2 = pd.DataFrame(columns = ['Stock Symbol','Bought at','Investment'])
    return dfl2

def make_a_move(df3,dfl,path,num_investments = 2):
    if dfl.shape[0] == 0:
        balance = 1000
    else:
        balance = dfl['Investment'].sum()
    df3 = df3.iloc[:num_investments,]
    df3['Investment'] = balance/num_investments
    dflf = df3[['Stock Symbol','last_day_value','Investment']]
    dflf.columns = ['Stock Symbol','Bought at','Investment']
    dflf.to_csv(path)
    return balance

df = read_stock_data(path = src+"main_data.csv").dropna()
l = [['Date','Balance']]
date = '2020-01-01'
num_investments = 2

ls = np.unique(df['Stock Symbol'])

while date < df['Date'].max():
    print(date)
    try:
        dfb = pd.read_csv(src+'balances.csv')
        date = (pd.to_datetime(dfb['Date'].max(),format='%Y-%m-%d') + pd.DateOffset(1)).strftime('%Y-%m-%d')
    except:
        dfb = pd.DataFrame(columns = ['Date','Balance'])
        date = '2020-01-01'

    df3 = create_decision_data(df, date, ls)



    dfl2 = update_ledger(df3, path = src+'ledger.csv')
    balance = make_a_move(df3,dfl2,num_investments = num_investments, path = src+'ledger.csv')
    l.append([date,balance])
    
    df4 = pd.DataFrame(l)
    df4.columns = df4.iloc[0]
    df4 = df4.drop(df4.index[0])
    
    df4.to_csv(src+'balances.csv')
    print(date,df['Date'].max())
    print('Next iteration :',date < df['Date'].max())

df3





y_train.shape

#importing required libraries
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM
import itertools
import numpy as np
import matplotlib.pyplot as plt
# warnings.filterwarnings("ignore")
plt.style.use('fivethirtyeight')
import pandas as pd
import statsmodels.api as sm
import matplotlib
from pylab import rcParams
# from pmdarima.arima import auto_arima


#read the file
df = pd.read_csv(src+'main_data.csv')
df=df.loc[df['Stock Symbol']=='GGB',['Date','Close']]

df['Date'] = pd.to_datetime(df.Date,format='%Y-%m-%d')
df.index = df['Date']

#creating dataframe
data = df.sort_index(ascending=True, axis=0)
new_data = pd.DataFrame(index=range(0,len(df)),columns=['Date', 'Close'])
for i in range(0,len(data)):
    new_data['Date'][i] = data['Date'][i]
    new_data['Close'][i] = data['Close'][i]

#setting index
new_data.index = new_data.Date
new_data.drop('Date', axis=1, inplace=True)

#creating train and test sets
dataset = new_data.values

train = dataset[0:987,:]
valid = dataset[987:,:]

#converting dataset into x_train and y_train
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(dataset)

x_train, y_train = [], []
for i in range(60,len(train)):
    x_train.append(scaled_data[i-60:i,0])
    y_train.append(scaled_data[i,0])
x_train, y_train = np.array(x_train), np.array(y_train)

x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))

# create and fit the LSTM network
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],1)))
model.add(LSTM(units=50))
model.add(Dense(1))

model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=1, batch_size=1, verbose=2)

#predicting 246 values, using past 60 from the train data
inputs = new_data[len(new_data) - len(valid) - 60:].values
inputs = inputs.reshape(-1,1)
inputs  = scaler.transform(inputs)

X_test = []
for i in range(60,inputs.shape[0]):
    X_test.append(inputs[i-60:i,0])
X_test = np.array(X_test)

X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))
closing_price = model.predict(X_test)
closing_price = scaler.inverse_transform(closing_price)

rms=np.sqrt(np.mean(np.power((valid-closing_price),2)))
rms

closing_price

#for plotting
train = new_data[:987]
valid = new_data[987:]
valid['Predictions'] = closing_price
plt.plot(train['Close'])
plt.plot(valid[['Close','Predictions']])



X_test.shape

scaler.inverse_transform(model.predict(X_test[]))